{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from SnakeEnv import SnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boque\\OneDrive\\Bureau\\1\\snake-RL\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.8     |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.6        |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 506         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006174394 |\n",
      "|    clip_fraction        | 0.011       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00466     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 65.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.6        |\n",
      "|    ep_rew_mean          | 14.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009812308 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 68.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.1        |\n",
      "|    ep_rew_mean          | 14.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011894457 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0765     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    value_loss           | 73.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38          |\n",
      "|    ep_rew_mean          | 20.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013476124 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 68          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x19ecd384e60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créez l'environnement\n",
    "env = SnakeEnv(grid_size=15)\n",
    "\n",
    "# Créez un agent PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entraînez l'agent\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/ppo_snake_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Cumulative Reward = 50.79999999999997\n",
      "Episode 2: Cumulative Reward = 19.89999999999998\n",
      "Episode 3: Cumulative Reward = 122.50000000000028\n",
      "Episode 4: Cumulative Reward = 0.20000000000000107\n",
      "Episode 5: Cumulative Reward = 10.999999999999986\n",
      "Episode 6: Cumulative Reward = 5.300000000000004\n",
      "Episode 7: Cumulative Reward = -1.299999999999999\n",
      "Episode 8: Cumulative Reward = -7.300000000000001\n",
      "Episode 9: Cumulative Reward = 2.8000000000000025\n",
      "Episode 10: Cumulative Reward = 3.4000000000000057\n",
      "Episode 11: Cumulative Reward = 11.600000000000001\n",
      "Episode 12: Cumulative Reward = 42.69999999999994\n",
      "Episode 13: Cumulative Reward = 5.400000000000002\n",
      "Episode 14: Cumulative Reward = 36.99999999999997\n",
      "Episode 15: Cumulative Reward = 45.699999999999946\n",
      "Episode 16: Cumulative Reward = 16.799999999999986\n",
      "Episode 17: Cumulative Reward = 0.4000000000000039\n",
      "Episode 18: Cumulative Reward = 94.00000000000017\n",
      "Episode 19: Cumulative Reward = 0.30000000000000426\n",
      "Episode 20: Cumulative Reward = 14.499999999999982\n",
      "Episode 21: Cumulative Reward = -3.799999999999999\n",
      "Episode 22: Cumulative Reward = 23.39999999999997\n",
      "Episode 23: Cumulative Reward = 5.3000000000000025\n",
      "Episode 24: Cumulative Reward = 6.999999999999993\n",
      "Episode 25: Cumulative Reward = -5.3999999999999995\n",
      "Episode 26: Cumulative Reward = 14.099999999999994\n",
      "Episode 27: Cumulative Reward = -3.0999999999999996\n",
      "Episode 28: Cumulative Reward = 65.3\n",
      "Episode 29: Cumulative Reward = -3.9000000000000004\n",
      "Episode 30: Cumulative Reward = 60.800000000000054\n",
      "Episode 31: Cumulative Reward = 116.70000000000024\n",
      "Episode 32: Cumulative Reward = 5.900000000000004\n",
      "Episode 33: Cumulative Reward = 0.20000000000000462\n",
      "Episode 34: Cumulative Reward = 1.3000000000000043\n",
      "Episode 35: Cumulative Reward = -3.9999999999999982\n",
      "Episode 36: Cumulative Reward = -6.9\n",
      "Episode 37: Cumulative Reward = -3.299999999999999\n",
      "Episode 38: Cumulative Reward = -11.1\n",
      "Episode 39: Cumulative Reward = 64.10000000000002\n",
      "Episode 40: Cumulative Reward = 14.299999999999976\n",
      "Episode 41: Cumulative Reward = 25.29999999999996\n",
      "Episode 42: Cumulative Reward = 0.7000000000000028\n",
      "Episode 43: Cumulative Reward = 28.399999999999977\n",
      "Episode 44: Cumulative Reward = 0.30000000000000426\n",
      "Episode 45: Cumulative Reward = 14.89999999999998\n",
      "Episode 46: Cumulative Reward = 20.499999999999986\n",
      "Episode 47: Cumulative Reward = 31.49999999999998\n",
      "Episode 48: Cumulative Reward = 9.999999999999996\n",
      "Episode 49: Cumulative Reward = 57.19999999999993\n",
      "Episode 50: Cumulative Reward = 4.000000000000005\n",
      "Episode 51: Cumulative Reward = 2.0000000000000018\n",
      "Episode 52: Cumulative Reward = 39.39999999999998\n",
      "Episode 53: Cumulative Reward = 4.5000000000000036\n",
      "Episode 54: Cumulative Reward = 7.300000000000001\n",
      "Episode 55: Cumulative Reward = 15.399999999999988\n",
      "Episode 56: Cumulative Reward = 19.29999999999996\n",
      "Episode 57: Cumulative Reward = -5.6\n",
      "Episode 58: Cumulative Reward = 25.09999999999998\n",
      "Episode 59: Cumulative Reward = 65.00000000000003\n",
      "Episode 60: Cumulative Reward = 7.599999999999994\n",
      "Episode 61: Cumulative Reward = 1.7000000000000046\n",
      "Episode 62: Cumulative Reward = 18.699999999999992\n",
      "Episode 63: Cumulative Reward = 5.700000000000005\n",
      "Episode 64: Cumulative Reward = 3.1000000000000014\n",
      "Episode 65: Cumulative Reward = -5.799999999999995\n",
      "Episode 66: Cumulative Reward = 7.499999999999982\n",
      "Episode 67: Cumulative Reward = 54.49999999999997\n",
      "Episode 68: Cumulative Reward = 3.800000000000006\n",
      "Episode 69: Cumulative Reward = 6.600000000000001\n",
      "Episode 70: Cumulative Reward = 49.19999999999993\n",
      "Episode 71: Cumulative Reward = 34.49999999999995\n",
      "Episode 72: Cumulative Reward = 73.50000000000011\n",
      "Episode 73: Cumulative Reward = 16.099999999999984\n",
      "Episode 74: Cumulative Reward = 34.89999999999997\n",
      "Episode 75: Cumulative Reward = 11.200000000000003\n",
      "Episode 76: Cumulative Reward = 20.199999999999985\n",
      "Episode 77: Cumulative Reward = 5.100000000000001\n",
      "Episode 78: Cumulative Reward = 15.899999999999991\n",
      "Episode 79: Cumulative Reward = -1.7999999999999972\n",
      "Episode 80: Cumulative Reward = 57.800000000000026\n",
      "Episode 81: Cumulative Reward = 72.00000000000001\n",
      "Episode 82: Cumulative Reward = -1.8999999999999968\n",
      "Episode 83: Cumulative Reward = 1.700000000000001\n",
      "Episode 84: Cumulative Reward = 16.499999999999975\n",
      "Episode 85: Cumulative Reward = -0.19999999999999574\n",
      "Episode 86: Cumulative Reward = 3.800000000000006\n",
      "Episode 87: Cumulative Reward = 7.699999999999992\n",
      "Episode 88: Cumulative Reward = -3.899999999999997\n",
      "Episode 89: Cumulative Reward = 52.39999999999993\n",
      "Episode 90: Cumulative Reward = 6.300000000000004\n",
      "Episode 91: Cumulative Reward = 12.899999999999995\n",
      "Episode 92: Cumulative Reward = 51.59999999999993\n",
      "Episode 93: Cumulative Reward = 55.59999999999994\n",
      "Episode 94: Cumulative Reward = 8.399999999999991\n",
      "Episode 95: Cumulative Reward = 72.10000000000011\n",
      "Episode 96: Cumulative Reward = 26.99999999999997\n",
      "Episode 97: Cumulative Reward = 51.49999999999994\n",
      "Episode 98: Cumulative Reward = 1.1000000000000014\n",
      "Episode 99: Cumulative Reward = 28.79999999999997\n",
      "Episode 100: Cumulative Reward = 29.99999999999997\n",
      "Tests terminés!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from SnakeEnv import SnakeEnv\n",
    "\n",
    "model = PPO.load(\"models/ppo_snake_model\")\n",
    "env = SnakeEnv(grid_size=15)\n",
    "\n",
    "for episode in range(100):  # Loop over episodes\n",
    "    obs = env.reset()  # Reset the environment at the start of each episode\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Uncomment this line to see the environment rendered (requires pygame)\n",
    "        # env.render()\n",
    "\n",
    "        # Get the action from the model\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)  # Take a step in the environment\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        # Optional: Add a delay to slow down rendering for visualization\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Cumulative Reward = {cumulative_reward}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Tests terminés!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.90000000000019\n",
      "Tests terminés!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = PPO.load(\"models/ppo_snake_model\")\n",
    "env = SnakeEnv(grid_size=15)\n",
    "\n",
    "obs = env.reset()  # Reset the environment at the start of each episode\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Uncomment this line to see the environment rendered (requires pygame)\n",
    "    # env.render()\n",
    "\n",
    "    # Get the action from the model\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)  # Take a step in the environment\n",
    "    cumulative_reward += reward\n",
    "\n",
    "    # Optional: Add a delay to slow down rendering for visualization\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(cumulative_reward)\n",
    "print(\"Tests terminés!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
